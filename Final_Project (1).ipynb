{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project\n",
    "Members: Lam Nguyen; Vu Trinh; Francesco Colombo; Minh Nguyen\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['condition', 'cylinders', 'VIN', 'drive', 'size', 'paint_color',\n",
      "       'county'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#TESTING PURPOSE ONLY - TEST FOR THRESHOLDS \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "veh_df = pd.read_csv('vehicles.csv')\n",
    "\n",
    "# 1. Missing Values Handling\n",
    "# Drop columns with high missing values (threshold currently set at 30%, or 0.30)\n",
    "threshold = 0.30  # example threshold of 30% missing values\n",
    "columns_to_drop = veh_df.columns[veh_df.isnull().mean() > threshold]\n",
    "veh_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print (columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "veh_df = pd.read_csv('vehicles.csv')\n",
    "\n",
    "# 1. Missing Values Handling\n",
    "# Drop columns with high missing values (threshold currently set at 30%, or 0.30)\n",
    "threshold = 0.30  # example threshold of 30% missing values\n",
    "columns_to_drop = veh_df.columns[veh_df.isnull().mean() > threshold]\n",
    "veh_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# print (columns_to_drop)\n",
    "# At 30% threshold, the following columns are \n",
    "# dropped:(['condition', 'cylinders', 'VIN', 'drive', 'size', 'paint_color','county'],\n",
    "    \n",
    "\n",
    "# 2. Column Removal\n",
    "columns_to_remove = ['url', 'region_url']  # columns mentioned as unnecessary\n",
    "veh_df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# 3. Data Type Conversion\n",
    "veh_df['year'] = pd.to_numeric(veh_df['year'], errors='coerce')\n",
    "veh_df['odometer'] = pd.to_numeric(veh_df['odometer'], errors='coerce')\n",
    "veh_df['price'] = pd.to_numeric(veh_df['price'], errors='coerce')\n",
    "\n",
    "# 4. Outlier Detection and Removal\n",
    "#  Removing outliers in 'price' column using z-score\n",
    "z_scores = np.abs(stats.zscore(veh_df['price']))\n",
    "veh_df = veh_df[(z_scores < 3)]\n",
    "\n",
    "# 5. Normalization/Scaling (Optional)\n",
    "scaler = MinMaxScaler()\n",
    "veh_df[['year', 'odometer']] = scaler.fit_transform(veh_df[['year', 'odometer']])\n",
    "\n",
    "\n",
    "# 6. Categorical Encoding ( One-Hot Encoding)\n",
    "# categorical_cols = ['manufacturer', 'model', 'fuel', 'title_status',\n",
    "                   # 'transmission']\n",
    "                   # # 'size', 'condition', 'type', 'drive', 'paint_color' --- might add these if heighten threshold in section 1.\n",
    "                    # such that these columns are not dropped.\n",
    "# veh_df = pd.get_dummies(veh_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 7. Feature Engineering (Extracting year from 'posting_date')\n",
    "veh_df['posting_date'] = pd.to_datetime(veh_df['posting_date'], utc=True)  # Specify 'utc=True' to handle mixed time zones\n",
    "veh_df['posting_year'] = veh_df['posting_date'].dt.year\n",
    "\n",
    "# 8. Handling Unusable Data (if necessary)\n",
    "# Check for irrelevant columns or rows and drop them as needed\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "veh_df.to_csv('cleaned_vehicles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lâm\\Downloads\\vehicles.csv\\Final_Project.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/L%C3%A2m/Downloads/vehicles.csv/Final_Project.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/L%C3%A2m/Downloads/vehicles.csv/Final_Project.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/L%C3%A2m/Downloads/vehicles.csv/Final_Project.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/L%C3%A2m/Downloads/vehicles.csv/Final_Project.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m stats\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/L%C3%A2m/Downloads/vehicles.csv/Final_Project.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[39m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_metadata_requests\u001b[39;00m \u001b[39mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bunch\u001b[39;00m \u001b[39mimport\u001b[39;00m Bunch\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclass_weight\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     18\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInvalidParameterError\u001b[39;00m(\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config \u001b[39mas\u001b[39;00m _get_config\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_array_api\u001b[39;00m \u001b[39mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m ComplexWarning\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_isfinite\u001b[39;00m \u001b[39mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_array_api_dispatch\u001b[39m(array_api_dispatch):\n\u001b[0;32m     13\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that array_api_compat is installed and NumPy version is compatible.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m    array_api_compat follows NEP29, which has a higher minimum NumPy version than\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m    scikit-learn.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\fixes.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreadpoolctl\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\__init__.py:608\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m \n\u001b[0;32m    604\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 608\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    609\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    610\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m cdist\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mndimage\u001b[39;00m \u001b[39mimport\u001b[39;00m _measurements\n\u001b[0;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_lib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_util\u001b[39;00m \u001b[39mimport\u001b[39;00m (check_random_state, MapWrapper, _get_nan,\n\u001b[0;32m     42\u001b[0m                               rng_integers, _rename_parameter, _contains_nan)\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\spatial\\__init__.py:116\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_plotutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    115\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_procrustes\u001b[39;00m \u001b[39mimport\u001b[39;00m procrustes\n\u001b[1;32m--> 116\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_geometric_slerp\u001b[39;00m \u001b[39mimport\u001b[39;00m geometric_slerp\n\u001b[0;32m    118\u001b[0m \u001b[39m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ckdtree, kdtree, qhull\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\spatial\\_geometric_slerp.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m euclidean\n\u001b[0;32m     11\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     12\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnpt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lâm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\spatial\\distance.py:118\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n\u001b[0;32m    116\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_lib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_util\u001b[39;00m \u001b[39mimport\u001b[39;00m _asarray_validated\n\u001b[1;32m--> 118\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distance_wrap\n\u001b[0;32m    119\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _hausdorff\n\u001b[0;32m    120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m norm\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "veh_df = pd.read_csv('vehicles.csv')\n",
    "\n",
    "# 1. Missing Values Handling\n",
    "threshold = 0.30  # Set threshold for missing values\n",
    "columns_to_drop = veh_df.columns[veh_df.isnull().mean() > threshold]\n",
    "veh_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# 2. Column Removal\n",
    "columns_to_remove = ['url', 'region_url']  # Remove unnecessary columns\n",
    "veh_df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# 3. Data Type Conversion\n",
    "veh_df['year'] = pd.to_numeric(veh_df['year'], errors='coerce')\n",
    "veh_df['odometer'] = pd.to_numeric(veh_df['odometer'], errors='coerce')\n",
    "veh_df['price'] = pd.to_numeric(veh_df['price'], errors='coerce')\n",
    "\n",
    "# 4. Outlier Detection and Removal\n",
    "z_scores = np.abs(stats.zscore(veh_df['price']))\n",
    "veh_df = veh_df[(z_scores < 3)]\n",
    "\n",
    "# 5. Normalization/Scaling (Optional)\n",
    "scaler = MinMaxScaler()\n",
    "veh_df[['year', 'odometer']] = scaler.fit_transform(veh_df[['year', 'odometer']])\n",
    "\n",
    "# 6. Categorical Encoding (One-Hot Encoding)\n",
    "categorical_cols = ['manufacturer', 'model', 'fuel', 'title_status', 'transmission']\n",
    "veh_df = pd.get_dummies(veh_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 7. Feature Engineering (Extracting year from 'posting_date')\n",
    "veh_df['posting_date'] = pd.to_datetime(veh_df['posting_date'], utc=True)\n",
    "veh_df['posting_year'] = veh_df['posting_date'].dt.year\n",
    "\n",
    "# 8. Handling Unusable Data (if necessary)\n",
    "# Check for irrelevant columns or rows and drop them as needed\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "veh_df.to_csv('cleaned_vehicles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['condition', 'cylinders', 'VIN', 'drive', 'size', 'paint_color',\n",
      "       'county'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "veh_df = pd.read_csv('vehicles.csv')\n",
    "\n",
    "# 1. Missing Values Handling\n",
    "threshold = 0.30  # Set threshold for missing values\n",
    "# CAUTION: If change this threshold, must re-check #2 to make sure all unwanted columns still dropped.\n",
    "columns_to_drop = veh_df.columns[veh_df.isnull().mean() > threshold]\n",
    "veh_df.drop(columns=columns_to_drop, inplace=True)\n",
    "print (columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['region', 'price', 'year', 'manufacturer', 'model', 'odometer', 'type',\n",
      "       'state'],\n",
      "      dtype='object')\n",
      "    region  price    year manufacturer                         model  \\\n",
      "50  auburn  38990  2020.0         ford        f150 supercrew cab xlt   \n",
      "51  auburn  22590  2017.0          ram    1500 regular cab tradesman   \n",
      "52  auburn  31590  2020.0        mazda               mx-5 miata club   \n",
      "53  auburn  27990  2020.0         ford     ranger supercab xl pickup   \n",
      "54  auburn  31590  2019.0     cadillac              xt4 sport suv 4d   \n",
      "55  auburn  19900  2004.0         ford               f250 super duty   \n",
      "56  auburn  16590  2016.0         jeep         renegade sport suv 4d   \n",
      "57  auburn  26990  2016.0         ford    f150 regular cab xl pickup   \n",
      "58  auburn  25590  2015.0          gmc       sierra 1500 regular cab   \n",
      "59  auburn  14000  2012.0        honda                       odyssey   \n",
      "60  auburn  28590  2018.0          ram  1500 quad cab express pickup   \n",
      "\n",
      "    odometer       type state  \n",
      "50   12231.0     pickup    al  \n",
      "51   39508.0     pickup    al  \n",
      "52    2195.0      other    al  \n",
      "53   10688.0     pickup    al  \n",
      "54   12102.0  hatchback    al  \n",
      "55   88000.0     pickup    al  \n",
      "56   35835.0      other    al  \n",
      "57   14230.0     pickup    al  \n",
      "58   35290.0     pickup    al  \n",
      "59   95000.0   mini-van    al  \n",
      "60   30047.0     pickup    al  \n"
     ]
    }
   ],
   "source": [
    "# 2. Column Removal\n",
    "columns_to_remove = ['id','url', 'region_url', 'image_url', 'id', 'fuel', 'transmission',\n",
    "                     'description', 'lat', 'long', 'posting_date', 'title_status']  # Remove unnecessary columns\n",
    "veh_df.drop(columns=columns_to_remove, inplace=True)\n",
    "print(veh_df.columns)\n",
    "# Assuming veh_df is the name of your DataFrame\n",
    "print(veh_df.iloc[50:61])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    region  price    year manufacturer                         model  \\\n",
      "50  auburn  38990  2020.0         ford        f150 supercrew cab xlt   \n",
      "51  auburn  22590  2017.0          ram    1500 regular cab tradesman   \n",
      "52  auburn  31590  2020.0        mazda               mx-5 miata club   \n",
      "53  auburn  27990  2020.0         ford     ranger supercab xl pickup   \n",
      "54  auburn  31590  2019.0     cadillac              xt4 sport suv 4d   \n",
      "55  auburn  19900  2004.0         ford               f250 super duty   \n",
      "56  auburn  16590  2016.0         jeep         renegade sport suv 4d   \n",
      "57  auburn  26990  2016.0         ford    f150 regular cab xl pickup   \n",
      "58  auburn  25590  2015.0          gmc       sierra 1500 regular cab   \n",
      "59  auburn  14000  2012.0        honda                       odyssey   \n",
      "60  auburn  28590  2018.0          ram  1500 quad cab express pickup   \n",
      "\n",
      "    odometer       type state  \n",
      "50   12231.0     pickup    al  \n",
      "51   39508.0     pickup    al  \n",
      "52    2195.0      other    al  \n",
      "53   10688.0     pickup    al  \n",
      "54   12102.0  hatchback    al  \n",
      "55   88000.0     pickup    al  \n",
      "56   35835.0      other    al  \n",
      "57   14230.0     pickup    al  \n",
      "58   35290.0     pickup    al  \n",
      "59   95000.0   mini-van    al  \n",
      "60   30047.0     pickup    al  \n"
     ]
    }
   ],
   "source": [
    "# 3. Data Type Conversion\n",
    "# Convert columns to numeric with coercion\n",
    "veh_df['year'] = pd.to_numeric(veh_df['year'], errors='coerce')\n",
    "veh_df['odometer'] = pd.to_numeric(veh_df['odometer'], errors='coerce')\n",
    "veh_df['price'] = pd.to_numeric(veh_df['price'], errors='coerce')\n",
    "\n",
    "# Test\n",
    "# print(veh_df.iloc[50:61])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Outlier Detection and Removal\n",
    "z_scores = np.abs(stats.zscore(veh_df['price']))\n",
    "veh_df = veh_df[(z_scores < 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Normalization/Scaling (Optional)\n",
    "scaler = MinMaxScaler()\n",
    "veh_df[['year', 'odometer']] = scaler.fit_transform(veh_df[['year', 'odometer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    region  price  year  manufacturer  model  odometer       type state\n",
      "50  auburn  38990    15             5   5886     47773     pickup    al\n",
      "51  auburn  22590    11            29  23832     17433     pickup    al\n",
      "52  auburn  31590    15            18  11817     41317      other    al\n",
      "53  auburn  27990    15             5  14026     39184     pickup    al\n",
      "54  auburn  31590    14            39  21496     47039  hatchback    al\n",
      "55  auburn  19900   110             5   6085     47618     pickup    al\n",
      "56  auburn  16590    10            13  14450     15014      other    al\n",
      "57  auburn  26990    10             5   5816     59340     pickup    al\n",
      "58  auburn  25590     9             6  16006     14629     pickup    al\n",
      "59  auburn  14000     6             8  12126     52280   mini-van    al\n",
      "60  auburn  28590    12            29  23377     10832     pickup    al\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from scipy import stats\n",
    "# 6. Categorical Encoding (Label Encoding)\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = ['manufacturer', 'model', 'year', 'odometer']\n",
    "for col in categorical_cols:\n",
    "    veh_df[col] = label_encoder.fit_transform(veh_df[col].astype(str))\n",
    "\n",
    "# Column 'condition' didn't have enough data, was deleted\n",
    "# print(veh_df.iloc[50:61])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part #6 (one-hot coding) might try \n",
    "Dimensionality Reduction Techniques: Explore techniques like PCA or feature hashing to reduce the dimensionality of high-cardinality categorical columns.\n",
    "Encoding Strategy: Consider using alternative encoding methods such as label encoding or grouping rare categories before one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "veh_df.to_csv('cleaned_vehicles.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
